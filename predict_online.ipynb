{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=300, fake_data=False, hidden1=70, hidden2=20, input_data_dir='/tmp/tensorflow/stock/input_data', learning_rate=0.03, log_dir='/tmp/tensorflow/stock/logs/fully_connected_feed', max_steps=300000)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tushare as ts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stock\n",
    "import data_collecter_sohu as sohu_dc\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import argparse\n",
    "\n",
    "FLAGS=argparse.Namespace()\n",
    "FLAGS.learning_rate=0.03\n",
    "FLAGS.max_steps=300000\n",
    "FLAGS.hidden1=70\n",
    "FLAGS.hidden2=20\n",
    "FLAGS.batch_size=300\n",
    "FLAGS.input_data_dir=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),'tensorflow/stock/input_data')\n",
    "FLAGS.log_dir=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),'tensorflow/stock/logs/fully_connected_feed')\n",
    "FLAGS.fake_data=False\n",
    "\n",
    "FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "  \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "  These placeholders are used as inputs by the rest of the model building\n",
    "  code and will be fed from the downloaded data in the .run() loop, below.\n",
    "\n",
    "  Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "\n",
    "  Returns:\n",
    "    features_placeholder: Features placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "  \"\"\"\n",
    "  # Note that the shapes of the placeholders match the shapes of the full\n",
    "  # image and label tensors, except the first dimension is now batch_size\n",
    "  # rather than the full size of the train or test data sets.\n",
    "  print('features placeholder shape %d %d'%(batch_size, stock.NUM_FEATURES))\n",
    "  features_placeholder = tf.placeholder(tf.float32, shape=(None,\n",
    "                                                         stock.NUM_FEATURES))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return features_placeholder, labels_placeholder\n",
    "\n",
    "def fill_feed_dict(features_feed, labels_feed, features_pl, labels_pl):\n",
    "  \"\"\"Fills the feed_dict for training the given step.\n",
    "\n",
    "  A feed_dict takes the form of:\n",
    "  feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "  }\n",
    "\n",
    "  Args:\n",
    "    data_set: The DataSet contains features and labels\n",
    "    features_pl: The features placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "\n",
    "  Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "  \"\"\"\n",
    "  feed_dict = {\n",
    "      features_pl: features_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict\n",
    "\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            features_placeholder,\n",
    "            labels_placeholder,\n",
    "            dataset):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    features_placeholder: The features placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: a dict contains features and labels\n",
    "  \"\"\"\n",
    "#  eval_data = tf.data.Dataset.from_tensor_slices((np.array(dataset['features']),np.array(dataset['lables'])))\n",
    "#  eval_data = eval_data.repeat()\n",
    "#  eval_data = eval_data.batch(FLAGS.batch_size)\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = 10\n",
    "  num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  next_element = iterator.get_next()\n",
    "  for step in xrange(steps_per_epoch):\n",
    "      x,y = sess.run(next_element)\n",
    "      feed_dict = fill_feed_dict(x,y,\n",
    "                               features_placeholder,\n",
    "                               labels_placeholder)\n",
    "      true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tensorflow/stock/logs/fully_connected_feed'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tr/bin/anaconda2/envs/python3/lib/python3.5/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2379 samples, 2151 for training, 237 for test \n",
      "batch size 300\n",
      "features placeholder shape 300 85\n",
      "Step 0: loss = 0.62 (0.009 sec)\n",
      "Step 1000: loss = 0.48 (0.001 sec)\n",
      "Step 2000: loss = 0.48 (0.002 sec)\n",
      "Step 3000: loss = 0.48 (0.001 sec)\n",
      "Step 4000: loss = 0.48 (0.001 sec)\n",
      "Step 5000: loss = 0.48 (0.001 sec)\n",
      "Step 6000: loss = 0.48 (0.001 sec)\n",
      "Step 7000: loss = 0.48 (0.002 sec)\n",
      "Step 8000: loss = 0.48 (0.001 sec)\n",
      "Step 9000: loss = 0.47 (0.001 sec)\n",
      "Step 10000: loss = 0.47 (0.001 sec)\n",
      "Step 11000: loss = 0.47 (0.002 sec)\n",
      "Step 12000: loss = 0.47 (0.001 sec)\n",
      "Step 13000: loss = 0.47 (0.001 sec)\n",
      "Step 14000: loss = 0.47 (0.001 sec)\n",
      "Step 15000: loss = 0.47 (0.001 sec)\n",
      "Step 16000: loss = 0.47 (0.002 sec)\n",
      "Step 17000: loss = 0.47 (0.001 sec)\n",
      "Step 18000: loss = 0.47 (0.001 sec)\n",
      "Step 19000: loss = 0.47 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2372  Precision @ 1: 0.7907\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2311  Precision @ 1: 0.7703\n",
      "Step 20000: loss = 0.47 (0.001 sec)\n",
      "Step 21000: loss = 0.47 (0.001 sec)\n",
      "Step 22000: loss = 0.47 (0.001 sec)\n",
      "Step 23000: loss = 0.47 (0.001 sec)\n",
      "Step 24000: loss = 0.47 (0.001 sec)\n",
      "Step 25000: loss = 0.47 (0.001 sec)\n",
      "Step 26000: loss = 0.47 (0.001 sec)\n",
      "Step 27000: loss = 0.47 (0.001 sec)\n",
      "Step 28000: loss = 0.47 (0.001 sec)\n",
      "Step 29000: loss = 0.47 (0.001 sec)\n",
      "Step 30000: loss = 0.47 (0.001 sec)\n",
      "Step 31000: loss = 0.47 (0.001 sec)\n",
      "Step 32000: loss = 0.47 (0.001 sec)\n",
      "Step 33000: loss = 0.47 (0.001 sec)\n",
      "Step 34000: loss = 0.47 (0.001 sec)\n",
      "Step 35000: loss = 0.47 (0.001 sec)\n",
      "Step 36000: loss = 0.47 (0.001 sec)\n",
      "Step 37000: loss = 0.48 (0.001 sec)\n",
      "Step 38000: loss = 0.48 (0.001 sec)\n",
      "Step 39000: loss = 0.48 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2393  Precision @ 1: 0.7977\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2324  Precision @ 1: 0.7747\n",
      "Step 40000: loss = 0.48 (0.001 sec)\n",
      "Step 41000: loss = 0.48 (0.001 sec)\n",
      "Step 42000: loss = 0.48 (0.001 sec)\n",
      "Step 43000: loss = 0.48 (0.001 sec)\n",
      "Step 44000: loss = 0.48 (0.001 sec)\n",
      "Step 45000: loss = 0.48 (0.001 sec)\n",
      "Step 46000: loss = 0.48 (0.001 sec)\n",
      "Step 47000: loss = 0.48 (0.001 sec)\n",
      "Step 48000: loss = 0.47 (0.001 sec)\n",
      "Step 49000: loss = 0.47 (0.001 sec)\n",
      "Step 50000: loss = 0.47 (0.001 sec)\n",
      "Step 51000: loss = 0.47 (0.001 sec)\n",
      "Step 52000: loss = 0.47 (0.001 sec)\n",
      "Step 53000: loss = 0.47 (0.001 sec)\n",
      "Step 54000: loss = 0.47 (0.001 sec)\n",
      "Step 55000: loss = 0.47 (0.001 sec)\n",
      "Step 56000: loss = 0.47 (0.001 sec)\n",
      "Step 57000: loss = 0.47 (0.001 sec)\n",
      "Step 58000: loss = 0.47 (0.001 sec)\n",
      "Step 59000: loss = 0.46 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2468  Precision @ 1: 0.8227\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2296  Precision @ 1: 0.7653\n",
      "Step 60000: loss = 0.46 (0.001 sec)\n",
      "Step 61000: loss = 0.46 (0.001 sec)\n",
      "Step 62000: loss = 0.46 (0.001 sec)\n",
      "Step 63000: loss = 0.46 (0.001 sec)\n",
      "Step 64000: loss = 0.45 (0.001 sec)\n",
      "Step 65000: loss = 0.45 (0.001 sec)\n",
      "Step 66000: loss = 0.45 (0.001 sec)\n",
      "Step 67000: loss = 0.45 (0.001 sec)\n",
      "Step 68000: loss = 0.45 (0.001 sec)\n",
      "Step 69000: loss = 0.44 (0.001 sec)\n",
      "Step 70000: loss = 0.44 (0.001 sec)\n",
      "Step 71000: loss = 0.44 (0.001 sec)\n",
      "Step 72000: loss = 0.44 (0.001 sec)\n",
      "Step 73000: loss = 0.43 (0.001 sec)\n",
      "Step 74000: loss = 0.43 (0.001 sec)\n",
      "Step 75000: loss = 0.43 (0.001 sec)\n",
      "Step 76000: loss = 0.43 (0.001 sec)\n",
      "Step 77000: loss = 0.42 (0.001 sec)\n",
      "Step 78000: loss = 0.42 (0.001 sec)\n",
      "Step 79000: loss = 0.42 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2570  Precision @ 1: 0.8567\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2221  Precision @ 1: 0.7403\n",
      "Step 80000: loss = 0.42 (0.002 sec)\n",
      "Step 81000: loss = 0.41 (0.001 sec)\n",
      "Step 82000: loss = 0.41 (0.001 sec)\n",
      "Step 83000: loss = 0.41 (0.001 sec)\n",
      "Step 84000: loss = 0.41 (0.001 sec)\n",
      "Step 85000: loss = 0.40 (0.001 sec)\n",
      "Step 86000: loss = 0.40 (0.001 sec)\n",
      "Step 87000: loss = 0.40 (0.001 sec)\n",
      "Step 88000: loss = 0.40 (0.001 sec)\n",
      "Step 89000: loss = 0.39 (0.001 sec)\n",
      "Step 90000: loss = 0.39 (0.001 sec)\n",
      "Step 91000: loss = 0.39 (0.001 sec)\n",
      "Step 92000: loss = 0.39 (0.001 sec)\n",
      "Step 93000: loss = 0.39 (0.001 sec)\n",
      "Step 94000: loss = 0.39 (0.001 sec)\n",
      "Step 95000: loss = 0.38 (0.001 sec)\n",
      "Step 96000: loss = 0.38 (0.001 sec)\n",
      "Step 97000: loss = 0.38 (0.001 sec)\n",
      "Step 98000: loss = 0.38 (0.001 sec)\n",
      "Step 99000: loss = 0.38 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2673  Precision @ 1: 0.8910\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2186  Precision @ 1: 0.7287\n",
      "Step 100000: loss = 0.38 (0.002 sec)\n",
      "Step 101000: loss = 0.38 (0.001 sec)\n",
      "Step 102000: loss = 0.38 (0.001 sec)\n",
      "Step 103000: loss = 0.37 (0.001 sec)\n",
      "Step 104000: loss = 0.37 (0.001 sec)\n",
      "Step 105000: loss = 0.37 (0.001 sec)\n",
      "Step 106000: loss = 0.37 (0.001 sec)\n",
      "Step 107000: loss = 0.37 (0.001 sec)\n",
      "Step 108000: loss = 0.37 (0.001 sec)\n",
      "Step 109000: loss = 0.37 (0.001 sec)\n",
      "Step 110000: loss = 0.37 (0.001 sec)\n",
      "Step 111000: loss = 0.37 (0.001 sec)\n",
      "Step 112000: loss = 0.37 (0.001 sec)\n",
      "Step 113000: loss = 0.36 (0.001 sec)\n",
      "Step 114000: loss = 0.36 (0.001 sec)\n",
      "Step 115000: loss = 0.36 (0.001 sec)\n",
      "Step 116000: loss = 0.36 (0.001 sec)\n",
      "Step 117000: loss = 0.36 (0.001 sec)\n",
      "Step 118000: loss = 0.35 (0.001 sec)\n",
      "Step 119000: loss = 0.35 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2746  Precision @ 1: 0.9153\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2119  Precision @ 1: 0.7063\n",
      "Step 120000: loss = 0.35 (0.001 sec)\n",
      "Step 121000: loss = 0.35 (0.001 sec)\n",
      "Step 122000: loss = 0.35 (0.001 sec)\n",
      "Step 123000: loss = 0.34 (0.001 sec)\n",
      "Step 124000: loss = 0.34 (0.001 sec)\n",
      "Step 125000: loss = 0.34 (0.001 sec)\n",
      "Step 126000: loss = 0.33 (0.001 sec)\n",
      "Step 127000: loss = 0.33 (0.001 sec)\n",
      "Step 128000: loss = 0.33 (0.001 sec)\n",
      "Step 129000: loss = 0.33 (0.001 sec)\n",
      "Step 130000: loss = 0.32 (0.001 sec)\n",
      "Step 131000: loss = 0.32 (0.001 sec)\n",
      "Step 132000: loss = 0.32 (0.001 sec)\n",
      "Step 133000: loss = 0.32 (0.001 sec)\n",
      "Step 134000: loss = 0.31 (0.001 sec)\n",
      "Step 135000: loss = 0.31 (0.001 sec)\n",
      "Step 136000: loss = 0.31 (0.001 sec)\n",
      "Step 137000: loss = 0.31 (0.001 sec)\n",
      "Step 138000: loss = 0.30 (0.001 sec)\n",
      "Step 139000: loss = 0.30 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2806  Precision @ 1: 0.9353\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2055  Precision @ 1: 0.6850\n",
      "Step 140000: loss = 0.30 (0.002 sec)\n",
      "Step 141000: loss = 0.29 (0.001 sec)\n",
      "Step 142000: loss = 0.29 (0.001 sec)\n",
      "Step 143000: loss = 0.29 (0.001 sec)\n",
      "Step 144000: loss = 0.29 (0.002 sec)\n",
      "Step 145000: loss = 0.28 (0.001 sec)\n",
      "Step 146000: loss = 0.28 (0.001 sec)\n",
      "Step 147000: loss = 0.28 (0.001 sec)\n",
      "Step 148000: loss = 0.27 (0.001 sec)\n",
      "Step 149000: loss = 0.27 (0.001 sec)\n",
      "Step 150000: loss = 0.27 (0.001 sec)\n",
      "Step 151000: loss = 0.27 (0.001 sec)\n",
      "Step 152000: loss = 0.26 (0.001 sec)\n",
      "Step 153000: loss = 0.26 (0.001 sec)\n",
      "Step 154000: loss = 0.26 (0.001 sec)\n",
      "Step 155000: loss = 0.26 (0.002 sec)\n",
      "Step 156000: loss = 0.26 (0.001 sec)\n",
      "Step 157000: loss = 0.25 (0.001 sec)\n",
      "Step 158000: loss = 0.25 (0.001 sec)\n",
      "Step 159000: loss = 0.25 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2878  Precision @ 1: 0.9593\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2093  Precision @ 1: 0.6977\n",
      "Step 160000: loss = 0.25 (0.001 sec)\n",
      "Step 161000: loss = 0.25 (0.001 sec)\n",
      "Step 162000: loss = 0.25 (0.001 sec)\n",
      "Step 163000: loss = 0.24 (0.001 sec)\n",
      "Step 164000: loss = 0.24 (0.001 sec)\n",
      "Step 165000: loss = 0.24 (0.001 sec)\n",
      "Step 166000: loss = 0.24 (0.001 sec)\n",
      "Step 167000: loss = 0.24 (0.001 sec)\n",
      "Step 168000: loss = 0.24 (0.001 sec)\n",
      "Step 169000: loss = 0.23 (0.001 sec)\n",
      "Step 170000: loss = 0.23 (0.001 sec)\n",
      "Step 171000: loss = 0.23 (0.001 sec)\n",
      "Step 172000: loss = 0.22 (0.001 sec)\n",
      "Step 173000: loss = 0.22 (0.001 sec)\n",
      "Step 174000: loss = 0.22 (0.001 sec)\n",
      "Step 175000: loss = 0.22 (0.001 sec)\n",
      "Step 176000: loss = 0.21 (0.001 sec)\n",
      "Step 177000: loss = 0.21 (0.001 sec)\n",
      "Step 178000: loss = 0.21 (0.001 sec)\n",
      "Step 179000: loss = 0.21 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2919  Precision @ 1: 0.9730\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2029  Precision @ 1: 0.6763\n",
      "Step 180000: loss = 0.20 (0.001 sec)\n",
      "Step 181000: loss = 0.20 (0.001 sec)\n",
      "Step 182000: loss = 0.20 (0.001 sec)\n",
      "Step 183000: loss = 0.20 (0.001 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 184000: loss = 0.19 (0.001 sec)\n",
      "Step 185000: loss = 0.19 (0.001 sec)\n",
      "Step 186000: loss = 0.19 (0.001 sec)\n",
      "Step 187000: loss = 0.19 (0.001 sec)\n",
      "Step 188000: loss = 0.18 (0.001 sec)\n",
      "Step 189000: loss = 0.18 (0.001 sec)\n",
      "Step 190000: loss = 0.18 (0.001 sec)\n",
      "Step 191000: loss = 0.18 (0.001 sec)\n",
      "Step 192000: loss = 0.17 (0.001 sec)\n",
      "Step 193000: loss = 0.17 (0.001 sec)\n",
      "Step 194000: loss = 0.17 (0.001 sec)\n",
      "Step 195000: loss = 0.17 (0.001 sec)\n",
      "Step 196000: loss = 0.17 (0.001 sec)\n",
      "Step 197000: loss = 0.16 (0.001 sec)\n",
      "Step 198000: loss = 0.16 (0.001 sec)\n",
      "Step 199000: loss = 0.16 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2947  Precision @ 1: 0.9823\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2029  Precision @ 1: 0.6763\n",
      "Step 200000: loss = 0.16 (0.002 sec)\n",
      "Step 201000: loss = 0.15 (0.001 sec)\n",
      "Step 202000: loss = 0.15 (0.001 sec)\n",
      "Step 203000: loss = 0.15 (0.001 sec)\n",
      "Step 204000: loss = 0.15 (0.001 sec)\n",
      "Step 205000: loss = 0.14 (0.001 sec)\n",
      "Step 206000: loss = 0.14 (0.001 sec)\n",
      "Step 207000: loss = 0.14 (0.001 sec)\n",
      "Step 208000: loss = 0.14 (0.001 sec)\n",
      "Step 209000: loss = 0.13 (0.001 sec)\n",
      "Step 210000: loss = 0.13 (0.001 sec)\n",
      "Step 211000: loss = 0.13 (0.001 sec)\n",
      "Step 212000: loss = 0.13 (0.001 sec)\n",
      "Step 213000: loss = 0.12 (0.001 sec)\n",
      "Step 214000: loss = 0.12 (0.001 sec)\n",
      "Step 215000: loss = 0.12 (0.001 sec)\n",
      "Step 216000: loss = 0.12 (0.001 sec)\n",
      "Step 217000: loss = 0.12 (0.001 sec)\n",
      "Step 218000: loss = 0.11 (0.001 sec)\n",
      "Step 219000: loss = 0.11 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2965  Precision @ 1: 0.9883\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 2016  Precision @ 1: 0.6720\n",
      "Step 220000: loss = 0.11 (0.002 sec)\n",
      "Step 221000: loss = 0.11 (0.001 sec)\n",
      "Step 222000: loss = 0.10 (0.001 sec)\n",
      "Step 223000: loss = 0.10 (0.001 sec)\n",
      "Step 224000: loss = 0.10 (0.001 sec)\n",
      "Step 225000: loss = 0.10 (0.001 sec)\n",
      "Step 226000: loss = 0.09 (0.001 sec)\n",
      "Step 227000: loss = 0.09 (0.001 sec)\n",
      "Step 228000: loss = 0.09 (0.001 sec)\n",
      "Step 229000: loss = 0.09 (0.001 sec)\n",
      "Step 230000: loss = 0.09 (0.001 sec)\n",
      "Step 231000: loss = 0.08 (0.001 sec)\n",
      "Step 232000: loss = 0.08 (0.001 sec)\n",
      "Step 233000: loss = 0.08 (0.001 sec)\n",
      "Step 234000: loss = 0.08 (0.001 sec)\n",
      "Step 235000: loss = 0.08 (0.001 sec)\n",
      "Step 236000: loss = 0.07 (0.001 sec)\n",
      "Step 237000: loss = 0.07 (0.001 sec)\n",
      "Step 238000: loss = 0.07 (0.001 sec)\n",
      "Step 239000: loss = 0.07 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2984  Precision @ 1: 0.9947\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 1941  Precision @ 1: 0.6470\n",
      "Step 240000: loss = 0.07 (0.002 sec)\n",
      "Step 241000: loss = 0.07 (0.001 sec)\n",
      "Step 242000: loss = 0.06 (0.001 sec)\n",
      "Step 243000: loss = 0.06 (0.001 sec)\n",
      "Step 244000: loss = 0.06 (0.001 sec)\n",
      "Step 245000: loss = 0.06 (0.001 sec)\n",
      "Step 246000: loss = 0.06 (0.001 sec)\n",
      "Step 247000: loss = 0.06 (0.001 sec)\n",
      "Step 248000: loss = 0.06 (0.001 sec)\n",
      "Step 249000: loss = 0.06 (0.001 sec)\n",
      "Step 250000: loss = 0.05 (0.001 sec)\n",
      "Step 251000: loss = 0.05 (0.001 sec)\n",
      "Step 252000: loss = 0.05 (0.001 sec)\n",
      "Step 253000: loss = 0.05 (0.001 sec)\n",
      "Step 254000: loss = 0.05 (0.001 sec)\n",
      "Step 255000: loss = 0.05 (0.001 sec)\n",
      "Step 256000: loss = 0.05 (0.001 sec)\n",
      "Step 257000: loss = 0.05 (0.001 sec)\n",
      "Step 258000: loss = 0.05 (0.001 sec)\n",
      "Step 259000: loss = 0.04 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2994  Precision @ 1: 0.9980\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 1942  Precision @ 1: 0.6473\n",
      "Step 260000: loss = 0.04 (0.001 sec)\n",
      "Step 261000: loss = 0.04 (0.001 sec)\n",
      "Step 262000: loss = 0.04 (0.001 sec)\n",
      "Step 263000: loss = 0.04 (0.001 sec)\n",
      "Step 264000: loss = 0.04 (0.001 sec)\n",
      "Step 265000: loss = 0.04 (0.001 sec)\n",
      "Step 266000: loss = 0.04 (0.001 sec)\n",
      "Step 267000: loss = 0.04 (0.001 sec)\n",
      "Step 268000: loss = 0.04 (0.001 sec)\n",
      "Step 269000: loss = 0.04 (0.001 sec)\n",
      "Step 270000: loss = 0.04 (0.001 sec)\n",
      "Step 271000: loss = 0.04 (0.001 sec)\n",
      "Step 272000: loss = 0.03 (0.001 sec)\n",
      "Step 273000: loss = 0.03 (0.001 sec)\n",
      "Step 274000: loss = 0.03 (0.001 sec)\n",
      "Step 275000: loss = 0.03 (0.001 sec)\n",
      "Step 276000: loss = 0.03 (0.001 sec)\n",
      "Step 277000: loss = 0.03 (0.001 sec)\n",
      "Step 278000: loss = 0.03 (0.001 sec)\n",
      "Step 279000: loss = 0.03 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2999  Precision @ 1: 0.9997\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 1930  Precision @ 1: 0.6433\n",
      "Step 280000: loss = 0.03 (0.001 sec)\n",
      "Step 281000: loss = 0.03 (0.001 sec)\n",
      "Step 282000: loss = 0.03 (0.001 sec)\n",
      "Step 283000: loss = 0.03 (0.001 sec)\n",
      "Step 284000: loss = 0.03 (0.001 sec)\n",
      "Step 285000: loss = 0.03 (0.001 sec)\n",
      "Step 286000: loss = 0.03 (0.001 sec)\n",
      "Step 287000: loss = 0.03 (0.001 sec)\n",
      "Step 288000: loss = 0.03 (0.001 sec)\n",
      "Step 289000: loss = 0.03 (0.001 sec)\n",
      "Step 290000: loss = 0.02 (0.001 sec)\n",
      "Step 291000: loss = 0.02 (0.001 sec)\n",
      "Step 292000: loss = 0.02 (0.001 sec)\n",
      "Step 293000: loss = 0.02 (0.001 sec)\n",
      "Step 294000: loss = 0.02 (0.001 sec)\n",
      "Step 295000: loss = 0.02 (0.001 sec)\n",
      "Step 296000: loss = 0.02 (0.001 sec)\n",
      "Step 297000: loss = 0.02 (0.001 sec)\n",
      "Step 298000: loss = 0.02 (0.001 sec)\n",
      "Step 299000: loss = 0.02 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 3000  Num correct: 2999  Precision @ 1: 0.9997\n",
      "Test Data Eval:\n",
      "Num examples: 3000  Num correct: 1930  Precision @ 1: 0.6433\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train STOCk for a number of steps.\"\"\"\n",
    "  #df_002024=ts.get_hist_data('002024',start='2014-01-01')\n",
    "if True:\n",
    "  df_002024=sohu_dc.get_hist_data('002024','20080101','20171204')\n",
    "  df_002024_norm=(df_002024-df_002024.min())/(df_002024.max()-df_002024.min())\n",
    "\n",
    "  market_data={'train':{},'test':{}}\n",
    "  market_data['train']['features']=[]\n",
    "  market_data['train']['lables']=[]\n",
    "  market_data['test']['features']=[]\n",
    "  market_data['test']['lables']=[]\n",
    "  features=[]\n",
    "  lables=[]\n",
    "\n",
    "  for index in range(df_002024.index.size):\n",
    "      slice=df_002024_norm[df_002024.index.size-6-index:df_002024.index.size-index]\n",
    "      if(slice.index.size<6):\n",
    "          break\n",
    "      features.append(slice[1:6].values.flatten())\n",
    "      if (slice.close[0]-slice.close[1])/slice.close[1]>0.03:\n",
    "          lables.append(0)\n",
    "      else:\n",
    "          lables.append(1) \n",
    "\n",
    "  test_indexes=np.random.randint(len(features),size=len(features)//10, dtype='int')\n",
    "  train_indexes=[x for x in range(len(features)) if x not in test_indexes]\n",
    "  print('total %d samples, %d for training, %d for test '%(len(features), len(train_indexes), len(test_indexes)))\n",
    "  \n",
    "  market_data['train']['features'] = [features[i] for i in train_indexes]\n",
    "  market_data['train']['lables'] = [lables[i] for i in train_indexes]\n",
    "  market_data['test']['features'] = [features[i] for i in test_indexes]\n",
    "  market_data['test']['lables'] = [lables[i] for i in test_indexes]\n",
    "\n",
    "  # Tell TensorFlow that the model will be built into the default Graph.\n",
    "  with tf.Graph().as_default():\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((np.array(market_data['train']['features']),np.array(market_data['train']['lables'])))\n",
    "    dataset_test = tf.data.Dataset.from_tensor_slices((np.array(market_data['test']['features']),np.array(market_data['test']['lables'])))\n",
    "\n",
    "    print('batch size %d'%(FLAGS.batch_size))\n",
    "    dataset_train = dataset_train.repeat().batch(FLAGS.batch_size)\n",
    "    dataset_test = dataset_test.repeat().batch(FLAGS.batch_size)\n",
    "    iterator = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                           dataset_train.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "    training_init_op = iterator.make_initializer(dataset_train)\n",
    "    #validation_init_op = iterator.make_initializer(dataset_test)\n",
    "    # Generate placeholders for the images and labels.\n",
    "    features_placeholder, labels_placeholder = placeholder_inputs(\n",
    "        FLAGS.batch_size)\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = stock.inference(features_placeholder,\n",
    "                             FLAGS.hidden1,\n",
    "                             FLAGS.hidden2)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss = stock.loss(logits, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op = stock.training(loss, FLAGS.learning_rate)\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = stock.evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "    sess.run(training_init_op)\n",
    "    # Start the training loop.\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        x,y=sess.run(next_element)\n",
    "        feed_dict = fill_feed_dict(x,y,\n",
    "                                 features_placeholder,\n",
    "                                 labels_placeholder)\n",
    "                # Run one step of the model.  The return values are the activations\n",
    "                # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "                # inspect the values of your Ops or variables, you may include them\n",
    "                # in the list passed to sess.run() and the value tensors will be\n",
    "                # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                               feed_dict=feed_dict)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if step % 1000 == 0:       \n",
    "                # Write the summaries and print an overview fairly often.\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                # Update the events file.\n",
    "                summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()\n",
    "                # reinit the iterator\n",
    "                sess.run(training_init_op)\n",
    "\n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 20000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "            checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "        # Evaluate against the training set.\n",
    "            print('Training Data Eval:')\n",
    "            do_eval(sess,\n",
    "                eval_correct,\n",
    "                features_placeholder,\n",
    "                labels_placeholder,\n",
    "                dataset_train)\n",
    "        # Evaluate against the validation set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                eval_correct,\n",
    "                features_placeholder,\n",
    "                labels_placeholder,\n",
    "                dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=sess.run(logits,{features_placeholder:features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2379 4758 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "date=[]\n",
    "for index in range(df_002024.index.size):\n",
    "    slice=df_002024[df_002024.index.size-6-index:df_002024.index.size-index]\n",
    "    if(slice.index.size<6):\n",
    "        break\n",
    "    date.append(slice.index[0])\n",
    "\n",
    "print(len(date),s.size,type(s))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20080123 0.998933 0.00106687\n",
      "20080131 0.999878 0.000122036\n",
      "20080204 0.998765 0.00123489\n",
      "20080218 1.0 3.69176e-07\n",
      "20080225 0.959243 0.0407574\n",
      "20080303 0.99999 9.78035e-06\n",
      "20080321 0.999997 3.02802e-06\n",
      "20080403 0.993805 0.00619504\n",
      "20080407 1.0 3.32222e-07\n",
      "20080423 0.99529 0.00470984\n",
      "20080424 0.998585 0.00141527\n",
      "20080430 0.999196 0.000803701\n",
      "20080508 0.999947 5.29751e-05\n",
      "20080513 0.995297 0.00470312\n",
      "20080528 0.999881 0.000119235\n",
      "20080624 0.999073 0.000927076\n",
      "20080625 0.998849 0.00115113\n",
      "20080707 0.994837 0.00516258\n",
      "20080708 0.996203 0.00379651\n",
      "20080710 1.0 5.79174e-10\n",
      "20080724 0.995444 0.00455624\n",
      "20080814 1.0 2.09984e-08\n",
      "20080819 0.999702 0.000297667\n",
      "20080820 0.998145 0.00185457\n",
      "20080903 0.999995 4.94619e-06\n",
      "20080912 0.999945 5.55586e-05\n",
      "20080919 0.999563 0.000436912\n",
      "20080922 0.998279 0.00172111\n",
      "20080925 0.998002 0.00199803\n",
      "20081020 0.999818 0.000182226\n",
      "20081021 1.0 5.97128e-09\n",
      "20081028 0.997291 0.00270879\n",
      "20081030 0.999913 8.73552e-05\n",
      "20081105 0.997481 0.0025195\n",
      "20081110 0.999979 2.1266e-05\n",
      "20081113 0.994815 0.00518518\n",
      "20081114 0.996205 0.00379517\n",
      "20081117 0.998262 0.0017377\n",
      "20081119 0.998052 0.00194805\n",
      "20081125 0.999999 7.21232e-07\n",
      "20081127 0.999997 2.96805e-06\n",
      "20081204 0.996717 0.00328321\n",
      "20081208 0.999944 5.63697e-05\n",
      "20081216 0.996737 0.0032633\n",
      "20081225 0.998558 0.0014425\n",
      "20090114 0.997829 0.00217099\n",
      "20090119 0.992403 0.00759656\n",
      "20090203 1.0 1.65628e-08\n",
      "20090204 0.996889 0.00311094\n",
      "20090206 0.999822 0.000178114\n",
      "20090213 0.999999 8.88151e-07\n",
      "20090220 0.997091 0.00290923\n",
      "20090223 0.999998 1.89296e-06\n",
      "20090304 1.0 4.78194e-09\n",
      "20090306 0.999994 5.82417e-06\n",
      "20090317 0.997453 0.00254663\n",
      "20090318 0.999618 0.000382135\n",
      "20090319 0.993056 0.00694383\n",
      "20090324 0.996334 0.00366652\n",
      "20090401 0.998859 0.00114058\n",
      "20090402 0.999998 1.63734e-06\n",
      "20090420 0.995491 0.00450941\n",
      "20090429 0.989141 0.0108589\n",
      "20090430 0.991021 0.0089788\n",
      "20090504 0.999927 7.34814e-05\n",
      "20090513 0.999121 0.000879443\n",
      "20090527 0.66468 0.33532\n",
      "20090609 0.906014 0.0939863\n",
      "20090610 0.961215 0.0387851\n",
      "20090714 0.968445 0.0315553\n",
      "20090720 0.977979 0.0220213\n",
      "20090728 0.998481 0.00151869\n",
      "20090804 0.992779 0.00722045\n",
      "20090805 1.0 2.7709e-09\n",
      "20090810 0.999877 0.000123311\n",
      "20090811 0.999603 0.000397113\n",
      "20090820 0.997453 0.00254737\n",
      "20090821 1.0 8.69364e-09\n",
      "20090824 0.999979 2.15122e-05\n",
      "20090826 0.98894 0.0110602\n",
      "20090903 0.992796 0.0072041\n",
      "20090907 0.996799 0.00320064\n",
      "20090908 0.997014 0.00298575\n",
      "20090911 0.983544 0.0164564\n",
      "20090917 0.830591 0.169409\n",
      "20090921 0.999875 0.000124742\n",
      "20091009 0.972332 0.0276679\n",
      "20091012 0.990929 0.00907066\n",
      "20091102 0.855805 0.144195\n",
      "20091112 0.958694 0.0413064\n",
      "20091113 0.974745 0.0252552\n",
      "20091123 0.999996 4.03544e-06\n",
      "20091125 0.998592 0.00140798\n",
      "20091207 0.990555 0.00944457\n",
      "20091210 0.990011 0.0099892\n",
      "20091223 0.999997 3.24891e-06\n",
      "20091224 0.990277 0.00972261\n",
      "20091229 0.984868 0.0151322\n",
      "20100114 0.957704 0.0422959\n",
      "20100118 0.99325 0.00674998\n",
      "20100208 0.967199 0.0328011\n",
      "20100212 0.906197 0.0938032\n",
      "20100225 0.725432 0.274568\n",
      "20100301 0.997857 0.00214266\n",
      "20100421 0.996066 0.00393367\n",
      "20100428 0.999401 0.000598955\n",
      "20100510 0.998825 0.00117491\n",
      "20100513 0.973274 0.0267265\n",
      "20100518 0.993753 0.00624741\n",
      "20100520 0.999994 5.98687e-06\n",
      "20100521 0.999799 0.000200817\n",
      "20100524 1.0 1.53949e-08\n",
      "20100527 0.999435 0.000565459\n",
      "20100604 0.937142 0.062858\n",
      "20100609 0.977008 0.022992\n",
      "20100621 0.974189 0.0258106\n",
      "20100622 0.995306 0.00469435\n",
      "20100702 0.987729 0.0122709\n",
      "20100706 0.99254 0.00745995\n",
      "20100709 0.978304 0.0216958\n",
      "20100719 0.910603 0.0893968\n",
      "20100720 0.962152 0.0378476\n",
      "20100728 0.925575 0.0744246\n",
      "20100802 0.913038 0.086962\n",
      "20100813 0.968189 0.0318111\n",
      "20100816 0.913324 0.0866755\n",
      "20100817 0.962121 0.0378789\n",
      "20100824 0.999924 7.55737e-05\n",
      "20100830 0.932972 0.0670284\n",
      "20100913 0.924657 0.0753428\n",
      "20100921 0.942186 0.0578135\n",
      "20101008 0.993947 0.00605301\n",
      "20101019 0.991734 0.00826626\n",
      "20101021 0.599803 0.400197\n",
      "20101025 0.978554 0.0214457\n",
      "20101109 0.992207 0.00779338\n",
      "20101110 0.944417 0.0555831\n",
      "20101115 0.994113 0.00588673\n",
      "20101207 0.988824 0.0111763\n",
      "20101213 0.955905 0.0440954\n",
      "20101228 0.992847 0.00715319\n",
      "20101229 0.999932 6.76161e-05\n",
      "20110106 0.971754 0.0282459\n",
      "20110112 0.998929 0.00107089\n",
      "20110118 0.981035 0.0189653\n",
      "20110126 0.981307 0.018693\n",
      "20110210 0.87237 0.12763\n",
      "20110211 0.979912 0.0200879\n",
      "20110214 0.996861 0.00313901\n",
      "20110224 0.948642 0.0513579\n",
      "20110304 0.938124 0.0618757\n",
      "20110307 0.953318 0.0466822\n",
      "20110329 0.562718 0.437282\n",
      "20110407 0.974534 0.0254656\n",
      "20110413 0.947097 0.0529034\n",
      "20110506 0.970159 0.029841\n",
      "20110531 0.698239 0.301761\n",
      "20110607 0.966157 0.0338429\n",
      "20110617 0.863675 0.136325\n",
      "20110622 0.804021 0.195979\n",
      "20110718 0.919636 0.0803637\n",
      "20110811 0.999309 0.000690594\n",
      "20110825 0.788183 0.211817\n",
      "20110921 0.920736 0.079264\n",
      "20110927 0.99823 0.00176971\n",
      "20111012 0.985005 0.0149954\n",
      "20111017 1.0 9.10429e-08\n",
      "20111024 0.969588 0.0304121\n",
      "20111025 0.990426 0.00957414\n",
      "20111026 1.0 1.19671e-07\n",
      "20111028 0.980286 0.0197141\n",
      "20111213 0.948732 0.0512677\n",
      "20111215 0.975794 0.0242058\n",
      "20111216 0.99966 0.000340436\n",
      "20111230 0.944663 0.0553373\n",
      "20120109 0.971929 0.0280708\n",
      "20120110 0.987753 0.0122472\n",
      "20120117 1.0 9.67413e-08\n",
      "20120119 0.982404 0.0175956\n",
      "20120120 1.0 4.77043e-07\n",
      "20120201 0.977593 0.0224072\n",
      "20120202 0.997062 0.00293845\n",
      "20120208 0.829554 0.170446\n",
      "20120209 0.939956 0.0600441\n",
      "20120220 0.99189 0.00811033\n",
      "20120222 0.867287 0.132713\n",
      "20120223 0.792815 0.207185\n",
      "20120302 0.957845 0.0421551\n",
      "20120306 0.751885 0.248115\n",
      "20120307 0.942248 0.0577522\n",
      "20120309 0.99664 0.00336032\n",
      "20120313 0.930859 0.0691408\n",
      "20120315 1.0 2.74893e-07\n",
      "20120316 0.993457 0.00654301\n",
      "20120326 0.962456 0.0375442\n",
      "20120405 0.994841 0.00515866\n",
      "20120412 0.974385 0.0256147\n",
      "20120507 0.99402 0.00597998\n",
      "20120515 0.91638 0.08362\n",
      "20120517 0.968432 0.0315685\n",
      "20120528 0.987912 0.0120878\n",
      "20120611 0.998175 0.00182519\n",
      "20120613 0.955402 0.0445984\n",
      "20120615 0.739066 0.260934\n",
      "20120702 0.959137 0.0408625\n",
      "20120704 0.601112 0.398888\n",
      "20120706 0.987016 0.0129838\n",
      "20120718 0.987237 0.012763\n",
      "20120724 0.998249 0.00175144\n",
      "20120731 0.957172 0.042828\n",
      "20120801 0.95887 0.0411297\n",
      "20120803 0.998926 0.00107414\n",
      "20120806 0.997341 0.00265869\n",
      "20120807 0.993234 0.00676563\n",
      "20120809 0.950146 0.0498543\n",
      "20120815 0.994227 0.00577281\n",
      "20120828 0.990768 0.00923149\n",
      "20120829 0.969528 0.030472\n",
      "20120830 0.999996 3.98699e-06\n",
      "20120903 0.937891 0.0621085\n",
      "20120904 0.999979 2.04892e-05\n",
      "20120905 0.994039 0.00596077\n",
      "20120907 1.0 4.02296e-09\n",
      "20120911 0.999726 0.00027421\n",
      "20120918 0.999974 2.6335e-05\n",
      "20120919 0.999998 1.90204e-06\n",
      "20120920 0.787545 0.212455\n",
      "20120924 0.996055 0.00394504\n",
      "20120927 0.991115 0.00888531\n",
      "20120928 0.998788 0.00121233\n",
      "20121009 0.996506 0.0034941\n",
      "20121010 0.997766 0.00223421\n",
      "20121018 0.999757 0.000243126\n",
      "20121022 0.953904 0.0460965\n",
      "20121029 0.949065 0.0509352\n",
      "20121031 0.986868 0.0131325\n",
      "20121101 0.999997 2.60127e-06\n",
      "20121123 0.983519 0.0164814\n",
      "20121205 0.999683 0.000317405\n",
      "20121207 1.0 3.60979e-11\n",
      "20121210 0.998847 0.00115301\n",
      "20121217 0.996125 0.00387485\n",
      "20121219 0.997929 0.0020708\n",
      "20121228 0.996451 0.00354902\n",
      "20130104 0.986704 0.0132963\n",
      "20130107 0.999973 2.68254e-05\n",
      "20130110 0.999733 0.000267187\n",
      "20130114 0.995822 0.00417773\n",
      "20130115 0.999418 0.000581616\n",
      "20130118 0.993739 0.00626088\n",
      "20130123 0.997427 0.00257281\n",
      "20130128 0.999738 0.000262464\n",
      "20130220 0.984972 0.015028\n",
      "20130228 0.966634 0.0333661\n",
      "20130305 0.999985 1.49192e-05\n",
      "20130306 0.999997 2.91778e-06\n",
      "20130312 0.999831 0.000169193\n",
      "20130319 0.999992 7.65262e-06\n",
      "20130320 0.983275 0.0167249\n",
      "20130322 0.987421 0.0125793\n",
      "20130409 0.985234 0.0147657\n",
      "20130410 0.91008 0.0899196\n",
      "20130416 0.923544 0.0764561\n",
      "20130418 0.999928 7.19242e-05\n",
      "20130419 0.999928 7.20109e-05\n",
      "20130503 0.952428 0.047572\n",
      "20130506 0.998955 0.0010447\n",
      "20130509 0.987625 0.0123753\n",
      "20130515 0.996255 0.00374453\n",
      "20130516 0.968848 0.0311521\n",
      "20130517 0.986424 0.0135762\n",
      "20130520 0.944691 0.0553092\n",
      "20130521 0.974207 0.0257927\n",
      "20130529 0.98732 0.0126802\n",
      "20130605 0.992628 0.00737166\n",
      "20130614 1.0 6.41914e-09\n",
      "20130619 0.807396 0.192604\n",
      "20130625 0.999998 1.80262e-06\n",
      "20130626 1.0 1.95153e-11\n",
      "20130628 0.996864 0.00313592\n",
      "20130704 0.985874 0.0141263\n",
      "20130709 0.99999 1.0055e-05\n",
      "20130710 0.98223 0.0177703\n",
      "20130711 1.0 8.71261e-09\n",
      "20130715 0.990735 0.00926468\n",
      "20130716 0.999992 8.44444e-06\n",
      "20130717 0.73984 0.26016\n",
      "20130719 0.993273 0.00672709\n",
      "20130723 1.0 6.49121e-08\n",
      "20130725 1.0 3.1079e-11\n",
      "20130730 0.988889 0.0111113\n",
      "20130731 1.0 6.48183e-16\n",
      "20130801 0.999999 8.7751e-07\n",
      "20130802 0.998685 0.00131471\n",
      "20130805 0.986818 0.0131817\n",
      "20130806 0.99899 0.00101019\n",
      "20130808 0.999997 3.10633e-06\n",
      "20130814 0.999967 3.29404e-05\n",
      "20130819 1.0 1.07664e-09\n",
      "20130820 0.999997 2.84459e-06\n",
      "20130821 0.992088 0.0079121\n",
      "20130823 0.989927 0.0100727\n",
      "20130830 0.99909 0.000910031\n",
      "20130902 0.999993 6.90259e-06\n",
      "20130909 0.987459 0.0125409\n",
      "20130910 0.978926 0.0210739\n",
      "20130912 1.0 3.71123e-07\n",
      "20130916 0.999561 0.000439034\n",
      "20130917 1.0 9.356e-13\n",
      "20130918 0.999986 1.4026e-05\n",
      "20130923 0.998165 0.00183483\n",
      "20130924 0.995626 0.00437414\n",
      "20130930 1.0 5.87155e-10\n",
      "20131010 0.999634 0.000365574\n",
      "20131018 1.0 3.65552e-07\n",
      "20131021 0.999508 0.000492454\n",
      "20131029 0.999961 3.91619e-05\n",
      "20131030 0.990618 0.00938185\n",
      "20131105 1.0 6.12463e-08\n",
      "20131107 0.999943 5.69861e-05\n",
      "20131118 0.993918 0.0060824\n",
      "20131121 0.991946 0.00805408\n",
      "20131127 0.989487 0.0105129\n",
      "20131210 0.988634 0.0113661\n",
      "20131218 1.0 4.16539e-09\n",
      "20131220 0.994233 0.00576724\n",
      "20140103 1.0 7.49479e-13\n",
      "20140107 0.993622 0.00637826\n",
      "20140108 0.989879 0.0101206\n",
      "20140114 0.98487 0.0151304\n",
      "20140115 0.996519 0.00348068\n",
      "20140121 0.999999 1.3729e-06\n",
      "20140122 0.991556 0.00844356\n",
      "20140124 0.994567 0.00543354\n",
      "20140127 0.999975 2.47611e-05\n",
      "20140129 0.990028 0.00997225\n",
      "20140212 0.994647 0.00535325\n",
      "20140217 0.979256 0.0207443\n",
      "20140219 0.974146 0.0258536\n",
      "20140305 0.995206 0.00479398\n",
      "20140311 0.995586 0.00441438\n",
      "20140313 0.996111 0.00388938\n",
      "20140317 0.997013 0.00298697\n",
      "20140321 0.992724 0.00727618\n",
      "20140325 0.999993 7.41324e-06\n",
      "20140404 0.959527 0.0404726\n",
      "20140408 0.983283 0.0167175\n",
      "20140414 0.922614 0.0773865\n",
      "20140417 0.999605 0.000394704\n",
      "20140429 0.985142 0.0148581\n",
      "20140430 0.999418 0.000582288\n",
      "20140505 0.9981 0.00190005\n",
      "20140508 0.987346 0.012654\n",
      "20140512 0.967325 0.0326752\n",
      "20140516 1.0 6.89428e-08\n",
      "20140521 0.991226 0.00877372\n",
      "20140528 0.998787 0.00121259\n",
      "20140530 0.999964 3.55438e-05\n",
      "20140605 0.977008 0.0229924\n",
      "20140606 0.995516 0.00448421\n",
      "20140609 0.978634 0.0213663\n",
      "20140623 0.986749 0.0132508\n",
      "20140630 0.999999 1.42429e-06\n",
      "20140701 0.999942 5.77519e-05\n",
      "20140702 0.997142 0.00285828\n",
      "20140711 0.990309 0.00969081\n",
      "20140715 0.999585 0.000415331\n",
      "20140724 0.99983 0.000170178\n",
      "20140725 0.958391 0.0416091\n",
      "20140728 0.988628 0.0113716\n",
      "20140804 0.999625 0.000374609\n",
      "20140807 0.999983 1.73062e-05\n",
      "20140808 0.999449 0.000550813\n",
      "20140811 1.0 1.28033e-07\n",
      "20140814 0.998953 0.00104658\n",
      "20140815 0.999751 0.000248623\n",
      "20140818 0.998417 0.00158297\n",
      "20140820 0.972324 0.0276758\n",
      "20140821 0.995543 0.00445709\n",
      "20140901 0.999953 4.70467e-05\n",
      "20140905 0.998891 0.00110909\n",
      "20140912 0.999783 0.000216668\n",
      "20140915 1.0 1.92949e-07\n",
      "20140917 0.992573 0.00742695\n",
      "20140925 0.990516 0.00948398\n",
      "20141027 0.999978 2.20956e-05\n",
      "20141028 0.989607 0.0103934\n",
      "20141029 0.997623 0.00237708\n",
      "20141031 0.998254 0.0017458\n",
      "20141105 0.990288 0.0097122\n",
      "20141117 0.999999 1.41226e-06\n",
      "20141119 0.999999 5.34778e-07\n",
      "20141127 0.992929 0.0070713\n",
      "20141128 0.988997 0.0110033\n",
      "20141201 0.999384 0.000615964\n",
      "20141203 0.995626 0.00437391\n",
      "20141204 0.998368 0.00163241\n",
      "20141209 0.999603 0.000396541\n",
      "20141210 0.9977 0.00230002\n",
      "20141215 1.0 3.71914e-07\n",
      "20141217 1.0 1.71534e-11\n",
      "20141229 0.999535 0.000465122\n",
      "20150108 0.999736 0.000264046\n",
      "20150116 0.999999 1.4146e-06\n",
      "20150120 0.993948 0.00605184\n",
      "20150121 0.999995 5.46325e-06\n",
      "20150122 0.999867 0.000132551\n",
      "20150126 0.998131 0.00186917\n",
      "20150127 0.99191 0.00808992\n",
      "20150129 0.999945 5.47138e-05\n",
      "20150203 0.997779 0.00222074\n",
      "20150210 0.980475 0.0195249\n",
      "20150213 0.998582 0.00141816\n",
      "20150216 0.999957 4.32021e-05\n",
      "20150302 0.987724 0.0122757\n",
      "20150305 0.9995 0.000499721\n",
      "20150316 0.995678 0.00432233\n",
      "20150318 0.999983 1.66674e-05\n",
      "20150325 0.999177 0.000823101\n",
      "20150417 1.0 1.23843e-12\n",
      "20150421 0.996958 0.00304209\n",
      "20150422 1.0 2.39416e-07\n",
      "20150423 0.999815 0.000184997\n",
      "20150504 0.99714 0.00286029\n",
      "20150506 0.999706 0.000294222\n",
      "20150508 0.999907 9.33158e-05\n",
      "20150511 0.995661 0.00433875\n",
      "20150512 0.999986 1.42011e-05\n",
      "20150513 0.999586 0.000413545\n",
      "20150521 1.0 4.60545e-14\n",
      "20150526 0.999029 0.000970661\n",
      "20150529 1.0 3.66948e-09\n",
      "20150601 1.0 1.8639e-16\n",
      "20150602 0.995052 0.00494777\n",
      "20150605 0.999959 4.14558e-05\n",
      "20150608 0.999879 0.000120574\n",
      "20150609 0.998905 0.00109475\n",
      "20150611 0.999082 0.000918389\n",
      "20150623 0.994849 0.00515063\n",
      "20150630 0.999349 0.000651137\n",
      "20150706 0.996709 0.00329109\n",
      "20150710 0.999629 0.000370966\n",
      "20150713 1.0 4.3453e-10\n",
      "20150714 1.0 3.48731e-09\n",
      "20150716 1.0 3.07039e-08\n",
      "20150717 0.999583 0.000417048\n",
      "20150720 0.999462 0.000537703\n",
      "20150721 1.0 1.41253e-09\n",
      "20150729 0.996876 0.00312398\n",
      "20150811 0.997302 0.00269816\n",
      "20150812 0.996457 0.00354317\n",
      "20150813 0.999995 4.71895e-06\n",
      "20150814 0.999586 0.000413937\n",
      "20150827 1.0 3.36669e-10\n",
      "20150828 1.0 1.064e-10\n",
      "20150831 1.0 1.46068e-15\n",
      "20150902 0.998312 0.00168775\n",
      "20150907 0.999979 2.06577e-05\n",
      "20150908 1.0 5.42848e-25\n",
      "20150909 1.0 1.08398e-08\n",
      "20150916 0.999613 0.000386889\n",
      "20150921 1.0 8.72799e-10\n",
      "20150922 0.998875 0.00112526\n",
      "20151008 0.999697 0.000302648\n",
      "20151009 0.996527 0.0034726\n",
      "20151012 0.999994 6.31996e-06\n",
      "20151015 0.99977 0.000229559\n",
      "20151022 0.994214 0.00578566\n",
      "20151029 0.994535 0.0054653\n",
      "20151104 0.982011 0.0179885\n",
      "20151202 0.999975 2.46551e-05\n",
      "20151207 0.989432 0.0105682\n",
      "20151217 0.982252 0.0177479\n",
      "20151221 1.0 2.04428e-08\n",
      "20151230 1.0 1.03128e-12\n",
      "20160106 0.999415 0.000585085\n",
      "20160108 0.993708 0.0062917\n",
      "20160118 0.999891 0.000109384\n",
      "20160119 0.975333 0.0246673\n",
      "20160122 1.0 3.02891e-07\n",
      "20160129 0.99972 0.000279916\n",
      "20160201 0.999989 1.05182e-05\n",
      "20160202 0.986499 0.0135012\n",
      "20160204 0.966654 0.0333461\n",
      "20160216 0.999787 0.000213473\n",
      "20160224 0.999057 0.000943106\n",
      "20160229 0.999999 5.81339e-07\n",
      "20160301 0.997115 0.00288455\n",
      "20160302 0.994722 0.00527837\n",
      "20160311 0.99208 0.00792022\n",
      "20160314 0.987904 0.0120965\n",
      "20160317 0.996693 0.00330744\n",
      "20160318 0.97908 0.0209198\n",
      "20160330 0.99994 5.96334e-05\n",
      "20160405 0.999999 9.86291e-07\n",
      "20160408 0.999919 8.07348e-05\n",
      "20160503 0.988568 0.0114324\n",
      "20160516 0.980401 0.0195986\n",
      "20160603 0.994229 0.00577079\n",
      "20160606 0.985222 0.0147777\n",
      "20160615 0.998994 0.00100592\n",
      "20160628 0.999983 1.69233e-05\n",
      "20160808 0.995587 0.00441321\n",
      "20160812 0.996273 0.00372725\n",
      "20160817 0.995176 0.00482429\n",
      "20161010 0.76949 0.23051\n",
      "20161018 0.97358 0.02642\n",
      "20161024 0.899125 0.100875\n",
      "20161130 0.929292 0.0707076\n",
      "20161206 0.998928 0.00107247\n",
      "20161207 0.989994 0.010006\n",
      "20161213 0.988958 0.011042\n",
      "20161215 0.995549 0.0044514\n",
      "20170110 0.992965 0.00703529\n",
      "20170221 0.703349 0.296651\n",
      "20170412 0.980782 0.0192181\n",
      "20170502 0.994969 0.00503098\n",
      "20170525 0.631102 0.368898\n",
      "20170531 0.691081 0.308919\n",
      "20170609 0.869742 0.130258\n",
      "20170612 0.863157 0.136843\n",
      "20170626 0.835878 0.164122\n",
      "20170629 0.940626 0.0593742\n",
      "20170726 0.973938 0.0260625\n",
      "20170727 0.760782 0.239218\n",
      "20170801 0.997996 0.00200396\n",
      "20170814 0.907932 0.0920683\n",
      "20170816 0.948444 0.051556\n",
      "20170822 0.999974 2.65222e-05\n",
      "20170911 0.848248 0.151752\n",
      "20170918 0.979759 0.0202406\n",
      "20170920 0.95412 0.0458796\n",
      "20170928 0.987295 0.0127049\n",
      "20171009 0.997343 0.00265743\n",
      "20171013 0.971464 0.0285358\n",
      "20171023 0.97325 0.0267503\n",
      "20171026 0.952407 0.0475932\n",
      "20171030 0.999999 7.82588e-07\n",
      "20171106 0.97486 0.0251403\n",
      "20171107 0.988113 0.0118873\n"
     ]
    }
   ],
   "source": [
    "for i in range(s.shape[0]):\n",
    "    y = np.exp(s[i])\n",
    "    y1=y[0]/y.sum()\n",
    "    y2=y[1]/y.sum()\n",
    "    if(y1>y2):\n",
    "        print(date[i],y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=sohu_dc.get_hist_data('002024','20171124','20171208')\n",
    "test_norm=(test-test.min())/(test.max()-test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20171201', '20171204', '20171205', '20171206', '20171207', '20171208']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features=[]\n",
    "test_date=[]\n",
    "\n",
    "for index in range(df_002024.index.size):\n",
    "    slice=test_norm[test_norm.index.size-6-index:test_norm.index.size-index]\n",
    "    if(slice.index.size<6):\n",
    "        break\n",
    "    test_features.append(slice[1:6].values.flatten())\n",
    "    test_date.append(slice.index[0])\n",
    "test_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20171204 0.977891 0.022109\n"
     ]
    }
   ],
   "source": [
    "s=sess.run(logits,{features_placeholder:test_features})\n",
    "for i in range(s.shape[0]):\n",
    "    y = np.exp(s[i])\n",
    "    y1=y[0]/y.sum()\n",
    "    y2=y[1]/y.sum()\n",
    "    if(y1>y2):\n",
    "        print(test_date[i],y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=sess.run(logits,{features_placeholder:test_features})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
